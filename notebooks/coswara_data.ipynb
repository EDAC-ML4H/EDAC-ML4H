{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import json\n",
    "import shutil\n",
    "import librosa\n",
    "import librosa.display\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Union, List\n",
    "from tqdm import tqdm\n",
    "from scipy.io.wavfile import write\n",
    "from audiomentations import Compose\n",
    "from audiomentations import TimeStretch\n",
    "from audiomentations import PitchShift\n",
    "from audiomentations import Shift\n",
    "from audiomentations import Trim\n",
    "from audiomentations import Gain\n",
    "from audiomentations import PolarityInversion\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATA_DIR = '../../Coswara-Data/'\n",
    "EXTRACTED_DATA_DIR = 'Extracted_data'\n",
    "SUFFIX = 'shallow'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir(path:Union[str, Path]):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def move_recordings(ids:List[str],\n",
    "                    id_path_map:dict[str, Union[str, Path]],\n",
    "                    target_dir:Union[str, Path],\n",
    "                    rec_format:str='.wav'):\n",
    "\n",
    "    for rec_id in tqdm(ids):\n",
    "        if rec_id not in id_path_map: continue\n",
    "        \n",
    "        old_path = id_path_map[rec_id]\n",
    "        file_name = f'{rec_id}{rec_format}'\n",
    "        new_path = os.path.join(target_dir, file_name)\n",
    "        shutil.copy(old_path, new_path)\n",
    "\n",
    "def move_set(paths:List[str], target_dir:str):\n",
    "    for p in paths:\n",
    "        target_path = os.path.join(target_dir, *p.split('/')[-2:])\n",
    "        shutil.copy(p, target_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate postive and negative recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(data_dir, suffix='shallow'):\n",
    "    # Get cough paths\n",
    "    extracted_data_dir = 'Extracted_data'\n",
    "    recording_regex = fr'202*/*/cough-{suffix}.wav'\n",
    "    search_path = os.path.join(data_dir, extracted_data_dir, recording_regex)\n",
    "    paths = glob.glob(search_path)\n",
    "\n",
    "    # Create folders for files\n",
    "    n_dir = os.path.join(data_dir, 'data', suffix, 'n')\n",
    "    p_dir = os.path.join(data_dir, 'data', suffix, 'p')\n",
    "    mkdir(n_dir)\n",
    "    mkdir(p_dir)\n",
    "\n",
    "    # Read metadata\n",
    "    meta_data_path = os.path.join(data_dir, 'combined_data.csv')\n",
    "    meta_data = pd.read_csv(meta_data_path)\n",
    "\n",
    "    # Separate IDs based on class\n",
    "    n_classes = ['healthy',\n",
    "                 'no_resp_illness_exposed',\n",
    "                 'resp_illness_not_identified',\n",
    "                 'recovered_full']\n",
    "    \n",
    "    p_classes = ['positive_mild',\n",
    "                 'positive_moderate',\n",
    "                 'positive_asymp']\n",
    "\n",
    "    n_mask = meta_data.covid_status.isin(n_classes)\n",
    "    p_mask = meta_data.covid_status.isin(p_classes)\n",
    "    n_ids = meta_data[n_mask].id.to_list()\n",
    "    p_ids = meta_data[p_mask].id.to_list()\n",
    "\n",
    "    # Map id to path\n",
    "    id_path_map = dict()\n",
    "    for path in paths:\n",
    "        rec_id = path.split('/')[-2]\n",
    "        if rec_id in id_path_map:\n",
    "            print(f'Duplicate id :: {rec_id}')\n",
    "            continue\n",
    "        \n",
    "        id_path_map[rec_id] = path\n",
    "\n",
    "    # Separate recordings based on class\n",
    "    move_recordings(n_ids, id_path_map, n_dir)\n",
    "    move_recordings(p_ids, id_path_map, p_dir)\n",
    "\n",
    "    # Get metadata for recordings\n",
    "    n_id_mask = meta_data.id.isin(n_ids)\n",
    "    p_id_mask = meta_data.id.isin(p_ids)\n",
    "    np_meta_data = meta_data[p_id_mask | n_id_mask]\n",
    "    np_meta_data_path = os.path.join(data_dir, 'data', suffix, 'meta_data.csv')\n",
    "    np_meta_data.to_csv(np_meta_data_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data\n",
    "extract_data(DATA_DIR, suffix=SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data_dir:str, suffix:str=SUFFIX, test_size:float=0.15):\n",
    "\n",
    "    # Collect paths to recordings\n",
    "    search_pattern = fr'{suffix}/*/*.wav'\n",
    "    search_path = os.path.join(data_dir, 'data', search_pattern)\n",
    "    paths = glob.glob(search_path)\n",
    "\n",
    "    # Extract labels from paths\n",
    "    labels = list(map(lambda p: p.split('/')[-2], paths))\n",
    "\n",
    "    # Create train (includes valid) and test set split of 85:15\n",
    "    train_paths, test_paths, *_ = train_test_split(paths,\n",
    "                                                   labels,\n",
    "                                                   test_size=test_size,\n",
    "                                                   stratify=labels,\n",
    "                                                   random_state=7)\n",
    "\n",
    "    # Move split to different folders\n",
    "    train_dir = os.path.join(data_dir, 'data', suffix, 'train')\n",
    "    test_dir = os.path.join(data_dir, 'data', suffix, 'test')\n",
    "    mkdir(os.path.join(train_dir, 'n'))\n",
    "    mkdir(os.path.join(train_dir, 'p'))\n",
    "    mkdir(os.path.join(test_dir, 'n'))\n",
    "    mkdir(os.path.join(test_dir, 'p'))\n",
    "    move_set(train_paths, train_dir)\n",
    "    move_set(test_paths, test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augment sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_set(set_dir:str, set_label:str='p', sr:int=22050, extension:str='.wav'):\n",
    "    # Original data augmentation configuration from the Brogrammer's git repo\n",
    "    augment1 = Compose([\n",
    "        TimeStretch(min_rate=0.7, max_rate=1.4, p=0.9),\n",
    "        PitchShift(min_semitones=-2, max_semitones=4, p=1),\n",
    "        Shift(min_fraction=-0.5, max_fraction=0.5, p=0.8),\n",
    "        Trim(p=1),\n",
    "        Gain(p=1),\n",
    "        PolarityInversion(p=0.8)   \n",
    "        ])\n",
    "\n",
    "    # Same augmentation configuration with TimeStretch parameters set to default\n",
    "    augment2 = Compose([\n",
    "        TimeStretch(min_rate=0.8, max_rate=1.25, p=0.5),\n",
    "        PitchShift(min_semitones=-2, max_semitones=4, p=1),\n",
    "        Shift(min_fraction=-0.5, max_fraction=0.5, p=0.8),\n",
    "        Trim(p=1),\n",
    "        Gain(p=1),\n",
    "        PolarityInversion(p=0.8)   \n",
    "        ])\n",
    "\n",
    "    label_dir = os.path.join(set_dir, set_label)\n",
    "    paths = glob.glob(os.path.join(label_dir, fr'*{extension}'))\n",
    "\n",
    "    j = 0\n",
    "    for p in tqdm(paths):\n",
    "        try:\n",
    "            data, _ = librosa.load(p, sr=sr)\n",
    "\n",
    "            # First augmentation\n",
    "            data=augment1(data, sr)\n",
    "            write(os.path.join(label_dir, str(j) + extension), sr, data)\n",
    "            j += 1\n",
    "            \n",
    "            # Second augmentation\n",
    "            data=augment2(data, sr)\n",
    "            write(os.path.join(label_dir, str(j) + extension), sr, data)\n",
    "            j += 1\n",
    "            \n",
    "        except:\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment sets individually\n",
    "train_dir = os.path.join(DATA_DIR, 'data', SUFFIX, 'train')\n",
    "test_dir = os.path.join(DATA_DIR, 'data', SUFFIX, 'test')\n",
    "\n",
    "augment_set(train_dir)\n",
    "augment_set(test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(set_dir:str,\n",
    "                     duration:int=7,\n",
    "                     sample_rate:int=22050):\n",
    "\n",
    "    # Data collection parameters\n",
    "    recording_regex = r'*/*.wav'\n",
    "    search_path = os.path.join(set_dir, recording_regex)\n",
    "\n",
    "    # Collect paths to recordings to analyse\n",
    "    paths = glob.glob(search_path)\n",
    "\n",
    "    # Extract MFCCs\n",
    "    data = {\n",
    "        'mfcc': [],\n",
    "        'label': []\n",
    "        }\n",
    "    \n",
    "    for path in tqdm(paths):\n",
    "        try:\n",
    "            y, _ = librosa.load(path, sr=sample_rate)\n",
    "            y = librosa.util.fix_length(y, size=sample_rate * duration)\n",
    "            mfcc = librosa.feature.mfcc(y=y, n_mfcc=15, n_fft=2048, hop_length=512)\n",
    "            mfcc = mfcc.T\n",
    "            \n",
    "            label = re.split(r'/|\\\\', path)[-2]\n",
    "            \n",
    "            data['mfcc'].append(mfcc.tolist())\n",
    "            data['label'].append(label)\n",
    "            \n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    # Save features in a JSON file\n",
    "    json_path = os.path.join(set_dir, 'mfcc15_augdata.json')\n",
    "\n",
    "    with open(json_path, 'w') as fp:\n",
    "        json.dump(data, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features for train and test sets\n",
    "extract_features(train_dir)\n",
    "extract_features(test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split training set into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(DATA_DIR, 'data', SUFFIX, 'train')\n",
    "train_path = os.path.join(train_dir, 'mfcc15_augdata.json')\n",
    "with open(train_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract labels and MFCCs\n",
    "X = data['mfcc']\n",
    "y = data['label']\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, stratify=y, test_size=0.15/0.85, random_state=7)\n",
    "\n",
    "# Save sets\n",
    "train_data = {'mfcc': X_train, 'label': y_train}\n",
    "train_save_path = os.path.join(train_dir, 'train.json')\n",
    "with open(train_save_path, 'w') as fp:\n",
    "        json.dump(train_data, fp, indent=4)\n",
    "\n",
    "valid_data = {'mfcc': X_valid, 'label': y_valid}\n",
    "valid_save_path = os.path.join(train_dir, 'valid.json')\n",
    "with open(valid_save_path, 'w') as fp:\n",
    "        json.dump(valid_data, fp, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
